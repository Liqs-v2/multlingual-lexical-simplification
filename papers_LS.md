## List of Literature
- Aumiller, Dennis, und Michael Gertz. „UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?“ arXiv, 5. Januar 2023. https://doi.org/10.48550/arXiv.2301.01764.  
- Devlin, Jacob, Ming-Wei Chang, Kenton Lee, und Kristina Toutanova. „BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding“. arXiv, 24. Mai 2019. http://arxiv.org/abs/1810.04805.  
- „How to read a paper“, o. J.  
- Kojima, Takeshi, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, und Yusuke Iwasawa. „Large Language Models are Zero-Shot Reasoners“. arXiv, 29. Januar 2023. https://doi.org/10.48550/arXiv.2205.11916.  
- Lan, Yunshi, Xiang Li, Xin Liu, Yang Li, Wei Qin, und Weining Qian. „Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts“. arXiv, 15. November 2023. https://doi.org/10.48550/arXiv.2311.09050.  
- Li, Xiaofei, Daniel Wiechmann, Yu Qiao, und Elma Kerz. „MANTIS at TSAR-2022 Shared Task: Improved Unsupervised Lexical Simplification with Pretrained Encoders“. In Proceedings of the Workshop on Text Simplification, Accessibility, and Readability (TSAR-2022), herausgegeben von Sanja Štajner, Horacio Saggion, Daniel Ferrés, Matthew Shardlow, Kim Cheng Sheang, Kai North, Marcos Zampieri, und Wei Xu, 243–50. Abu Dhabi, United Arab Emirates (Virtual): Association for Computational Linguistics, 2022. https://doi.org/10.18653/v1/2022.tsar-1.27.  
- Lightman, Hunter, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, und Karl Cobbe. „Let’s Verify Step by Step“. arXiv, 31. Mai 2023. https://doi.org/10.48550/arXiv.2305.20050.  
- North, Kai, Tharindu Ranasinghe, Matthew Shardlow, und Marcos Zampieri. „Deep Learning Approaches to Lexical Simplification: A Survey“. arXiv, 19. Mai 2023. https://doi.org/10.48550/arXiv.2305.12000.  
- ———. „MultiLS: A Multi-task Lexical Simplification Framework“. arXiv, 22. Februar 2024. http://arxiv.org/abs/2402.14972.  
- North, Kai, Marcos Zampieri, und Matthew Shardlow. „Lexical Complexity Prediction: An Overview“. ACM Computing Surveys 55, Nr. 9 (30. September 2023): 1–42. https://doi.org/10.1145/3557885.  
- Omelianchuk, Kostiantyn, Vipul Raheja, und Oleksandr Skurzhanskyi. „Text Simplification by Tagging“. arXiv, 8. März 2021. https://doi.org/10.48550/arXiv.2103.05070.  
- Paetzold, Gustavo, und Lucia Specia. „Benchmarking Lexical Simplification Systems“. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC’16), herausgegeben von Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani, u. a., 3074–80. Portorož, Slovenia: European Language Resources Association (ELRA), 2016. https://aclanthology.org/L16-1491.  
- ———. „Unsupervised Lexical Simplification for Non-Native Speakers“. Proceedings of the AAAI Conference on Artificial Intelligence 30, Nr. 1 (5. März 2016). https://doi.org/10.1609/aaai.v30i1.9885.
- Qiang, Jipeng, Yun Li, Yi Zhu, Yunhao Yuan, Yang Shi, und Xindong Wu. „LSBert: Lexical Simplification Based on BERT“. IEEE/ACM Transactions on Audio, Speech, and Language Processing 29 (2021): 3064–76. https://doi.org/10.1109/TASLP.2021.3111589.  
- Qiang, Jipeng, Yun Li, Yi Zhu, Yunhao Yuan, und Xindong Wu. „Lexical Simplification with Pretrained Encoders“. Proceedings of the AAAI Conference on Artificial Intelligence 34, Nr. 05 (3. April 2020): 8649–56. https://doi.org/10.1609/aaai.v34i05.6389.  
- Saggion, Horacio, Sanja Štajner, Daniel Ferrés, Kim Cheng Sheang, Matthew Shardlow, Kai North, und Marcos Zampieri. „Findings of the TSAR-2022 Shared Task on Multilingual Lexical Simplification“. arXiv, 6. Februar 2023. https://doi.org/10.48550/arXiv.2302.02888.  
- Seneviratne, Sandaru, Eleni Daskalaki, und Hanna Suominen. „TextSimplifier: A Modular, Extensible, and Context Sensitive Simplification Framework for Improved Natural Language Understanding“. In Proceedings of the Second Workshop on Text Simplification, Accessibility and Readability, herausgegeben von Sanja Štajner, Horacio Saggio, Matthew Shardlow, und Fernando Alva-Manchego, 21–32. Varna, Bulgaria: INCOMA Ltd., Shoumen, Bulgaria, 2023. https://aclanthology.org/2023.tsar-1.3.  
- Sun, Renliang, Wei Xu, und Xiaojun Wan. „Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification“. arXiv, 21. Mai 2023. http://arxiv.org/abs/2305.12463.  
- Tan, Keren, Kangyang Luo, Yunshi Lan, Zheng Yuan, und Jinlong Shu. „An LLM-Enhanced Adversarial Editing System for Lexical Simplification“. arXiv, 22. März 2024. https://doi.org/10.48550/arXiv.2402.14704.  
- Taylor, Wilson L. „“Cloze Procedure”: A New Tool for Measuring Readability“. Journalism Quarterly 30, Nr. 4 (1. September 1953): 415–33. https://doi.org/10.1177/107769905303000401.  
- Vásquez-Rodríguez, Laura, Nhung Nguyen, Matthew Shardlow, und Sophia Ananiadou. „UoM&MMU at TSAR-2022 Shared Task: Prompt Learning for Lexical Simplification“. In Proceedings of the Workshop on Text Simplification, Accessibility, and Readability (TSAR-2022), herausgegeben von Sanja Štajner, Horacio Saggion, Daniel Ferrés, Matthew Shardlow, Kim Cheng Sheang, Kai North, Marcos Zampieri, und Wei Xu, 218–24. Abu Dhabi, United Arab Emirates (Virtual): Association for Computational Linguistics, 2022. https://doi.org/10.18653/v1/2022.tsar-1.23.  
- Wang, Jiarui, Richong Zhang, Junfan Chen, Jaein Kim, und Yongyi Mao. „Text Style Transferring via Adversarial Masking and Styled Filling“. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, herausgegeben von Yoav Goldberg, Zornitsa Kozareva, und Yue Zhang, 7654–63. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics, 2022. https://doi.org/10.18653/v1/2022.emnlp-main.521.  
- Wei, Jason, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, u. a. „Emergent Abilities of Large Language Models“. arXiv, 26. Oktober 2022. https://doi.org/10.48550/arXiv.2206.07682.  
- Wei, Jason, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, und Denny Zhou. „Chain-of-Thought Prompting Elicits Reasoning in Large Language Models“. arXiv, 10. Januar 2023. https://doi.org/10.48550/arXiv.2201.11903.  
- Yang, Chengxue, Zhijuan Wang, Yu Zhang, und Xiaobing Zhao. „A Dive into Lexical Simplification with Pre-Trained Model“. In 2023 4th International Conference on Machine Learning and Computer Application, 92–95. Hangzhou China: ACM, 2023. https://doi.org/10.1145/3650215.3650233.  
- Zhang, Zhuosheng, Aston Zhang, Mu Li, und Alex Smola. „Automatic Chain of Thought Prompting in Large Language Models“. arXiv, 7. Oktober 2022. https://doi.org/10.48550/arXiv.2210.03493.  
- Zhao, Tony Z., Eric Wallace, Shi Feng, Dan Klein, und Sameer Singh. „Calibrate Before Use: Improving Few-Shot Performance of Language Models“. arXiv, 10. Juni 2021. https://doi.org/10.48550/arXiv.2102.09690.  
- Zhou, Pei, Aman Madaan, Srividya Pranavi Potharaju, Aditya Gupta, Kevin R. McKee, Ari Holtzman, Jay Pujara, u. a. „How FaR Are Large Language Models From Agents with Theory-of-Mind?“ arXiv, 4. Oktober 2023. https://doi.org/10.48550/arXiv.2310.03051.  
- Zhou, Pei, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V. Le, Ed H. Chi, Denny Zhou, Swaroop Mishra, und Huaixiu Steven Zheng. „Self-Discover: Large Language Models Self-Compose Reasoning Structures“. arXiv, 5. Februar 2024. https://doi.org/10.48550/arXiv.2402.03620.  
