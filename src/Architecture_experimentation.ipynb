{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPJY30JuKIP3tCN1J0BOLoy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kSNr3cWzDK9Y","executionInfo":{"status":"ok","timestamp":1715590210234,"user_tz":-120,"elapsed":19307,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}},"outputId":"da2f3bbf-77a7-4808-c490-e46237f57f1c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/nlp_ss24/multilingual-lexical-simplification')\n","sys.path.append('/content/drive/MyDrive/nlp_ss24/multilingual-lexical-simplification/src')"],"metadata":{"id":"nKbKg35RESMm","executionInfo":{"status":"ok","timestamp":1715590212274,"user_tz":-120,"elapsed":246,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"tCkCeAkOC38M","executionInfo":{"status":"ok","timestamp":1715590754937,"user_tz":-120,"elapsed":260,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}}},"outputs":[],"source":["from lexical_simplifier import LexicalSimplifier\n","\n","\n","class GermanBertLexicalSimplifier(LexicalSimplifier):\n","    \"\"\"\n","    A German BERT based implementation of lexical simplification. Masks the given complex word with [MASK], adds other\n","    BERT specific tokens and generates a list of possible substitutions via the model predictions based on the prompt.\n","    \"\"\"\n","\n","    def __init__(self, model, pattern, exemplars):\n","        super().__init__(model, pattern, exemplars)\n","\n","    def generate_substitutions_for(self, complex_word):\n","        \"\"\"Generates a list of substitutions via the model predictions for the given complex word.\"\"\"\n","        raise NotImplementedError(\"Please implement this method in the subclass.\")"]},{"cell_type":"code","source":["from transformers import AutoModelForMaskedLM, AutoTokenizer\n","import torch\n","\n","# Load the tokenizer and model\n","tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n","model = AutoModelForMaskedLM.from_pretrained(\"dbmdz/bert-base-german-cased\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g1NDZFYOMxnh","executionInfo":{"status":"ok","timestamp":1715591149528,"user_tz":-120,"elapsed":3307,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}},"outputId":"5069b3ad-bb64-4290-afd3-a2531b27e777"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at dbmdz/bert-base-german-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["# Input text with a [MASK] token\n","bert_pattern = '{original_sentence} Die einfachere Version der vorigen Satzes ist: {sentence_with_complex_word_masked}'\n","text = 'Der Fluss wurde begradigt um mehr Baufläche zu schaffen.'\n","text_masked = 'Der Fluss wurde [MASK] um mehr Baufläche zu schaffen.'\n","\n","input_text = bert_patter.format(original_sentence=text, sentence_with_complex_word_masked)\n","\n","# Tokenize input text\n","inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n","\n","# Forward pass through the model\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Get predicted probabilities for the masked token\n","masked_index = inputs[\"input_ids\"].squeeze().tolist().index(tokenizer.mask_token_id)\n","probs = torch.nn.functional.softmax(outputs.logits[0, masked_index], dim=-1)\n","\n","# Get the top predictions\n","top_k = 5\n","top_k_tokens = torch.topk(probs, k=top_k).indices.tolist()\n","\n","# Convert token IDs back to tokens\n","predicted_tokens = [tokenizer.decode(token).strip() for token in top_k_tokens]\n","\n","print(\"Predicted tokens:\", predicted_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SWzCf4nSLX9T","executionInfo":{"status":"ok","timestamp":1715591442638,"user_tz":-120,"elapsed":846,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}},"outputId":"a9935e02-7c0c-45d8-a183-5b18c953da1b"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted tokens: ['gebaut', 'ausgebaut', 'angelegt', 'verlegt', 'vergrößert']\n"]}]},{"cell_type":"code","source":["bert_pattern = '{original_sentence} Die einfachere Version der vorigen Satzes ist: {sentence_with_complex_word_masked}'"],"metadata":{"id":"iyyccl28KDf4","executionInfo":{"status":"ok","timestamp":1715591213398,"user_tz":-120,"elapsed":242,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["german_bert_lexical_simplifier = GermanBertLexicalSimplifier(model, bert_pattern, None)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"MVlFMYSyJehv","executionInfo":{"status":"error","timestamp":1715591178926,"user_tz":-120,"elapsed":1858,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}},"outputId":"00623e17-7452-4053-9b51-8f2679053e0b"},"execution_count":15,"outputs":[{"output_type":"error","ename":"UserWarning","evalue":"No exemplars provided, using zero-shot mode.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUserWarning\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-446ec5d0c583>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgerman_bert_lexical_simplifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGermanBertLexicalSimplifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-2a5b1df02856>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, pattern, exemplars)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexemplars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexemplars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_substitutions_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomplex_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/nlp_ss24/multilingual-lexical-simplification/src/lexical_simplifier.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, pattern, exemplars)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexemplars\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexemplars\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No exemplars provided, using zero-shot mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUserWarning\u001b[0m: No exemplars provided, using zero-shot mode."]}]},{"cell_type":"code","source":[],"metadata":{"id":"BmYzDnZdKu-B"},"execution_count":null,"outputs":[]}]}