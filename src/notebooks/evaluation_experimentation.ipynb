{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3994,"status":"ok","timestamp":1717571535107,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"},"user_tz":-120},"id":"pcdIhYd6KWBl","outputId":"8198c66c-8a12-4111-f69b-eff39e1c259e"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm"],"metadata":{"id":"Owxjwke5S2Ud","executionInfo":{"status":"ok","timestamp":1717571535108,"user_tz":-120,"elapsed":5,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"xI6g7WXcKWBn","executionInfo":{"status":"ok","timestamp":1717571535108,"user_tz":-120,"elapsed":4,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}}},"source":["import sys\n","sys.path.append('/content/drive/MyDrive/nlp_ss24/multilingual-lexical-simplification')\n","sys.path.append('/content/drive/MyDrive/nlp_ss24/multilingual-lexical-simplification/src')"],"outputs":[]},{"cell_type":"code","source":["!git config --global user.name \"Tobias Lindenbauer\"\n","!git config --global user.email \"tobias.lindenbauer@tum.de\"\n","# Move to repo directory\n","%cd /content/drive/MyDrive/nlp_ss24/multilingual-lexical-simplification/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gh71OiXHDg9f","executionInfo":{"status":"ok","timestamp":1717572663435,"user_tz":-120,"elapsed":255,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}},"outputId":"726beccb-2ab0-4bc5-ce27-d949b4179656"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/nlp_ss24/multilingual-lexical-simplification\n"]}]},{"cell_type":"code","source":["!git pull"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KZbaRX94DiiL","executionInfo":{"status":"ok","timestamp":1717570457288,"user_tz":-120,"elapsed":25851,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}},"outputId":"449ae9e9-6213-41a9-e938-7db1abc673f2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["error: cannot pull with rebase: You have unstaged changes.\n","error: please commit or stash them.\n"]}]},{"cell_type":"markdown","source":["# LLM based"],"metadata":{"id":"0FBorNunEHuW"}},{"cell_type":"code","source":["!pip install accelerate flash-attn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpjpEBgOLnpS","executionInfo":{"status":"ok","timestamp":1717570644487,"user_tz":-120,"elapsed":92770,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}},"outputId":"c4bc3d4c-5c76-4e66-cead-e9e56111ef90"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate\n","  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting flash-attn\n","  Downloading flash_attn-2.5.9.post1.tar.gz (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.2)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Collecting einops (from flash-attn)\n","  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Building wheels for collected packages: flash-attn\n","  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for flash-attn: filename=flash_attn-2.5.9.post1-cp310-cp310-linux_x86_64.whl size=120889689 sha256=5022ba11d48bf74926da9c16260f4ea2b9bb7f4e29bdb4bd6e1383ad1c55d16f\n","  Stored in directory: /root/.cache/pip/wheels/cc/ad/f6/7ccf0238790d6346e9fe622923a76ec218e890d356b9a2754a\n","Successfully built flash-attn\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, flash-attn, accelerate\n","Successfully installed accelerate-0.30.1 einops-0.8.0 flash-attn-2.5.9.post1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"]}]},{"cell_type":"code","source":["from llm_lexical_simplifier import LLMLexicalSimplifier\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n","\n","torch.random.manual_seed(0)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"microsoft/Phi-3-mini-4k-instruct\",\n","    device_map=\"cuda\",\n","    torch_dtype=\"auto\",\n","    trust_remote_code=True,\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193,"referenced_widgets":["db4cdc6c752f472b873f40a96ffa8376","89fd983aa28b49429ebbadbad49858e1","15bd3262876244edb973433c02cb358a","9fb6f554ee164820944359a6dafa44ae","b217fcc4be144a67b32e7752ebae9e2c","375a8df081a84d85885146e76d80d467","129309f4318f4541a58d135f55810349","c3249b75df36474f81d75eb81b6753b9","ac6fb352152149409b9d9007d5419510","c406072ff5a64e919adcbf5e9ebf0258","12f748b910da4daa8800e3fa8b5bfbb7"]},"id":"XNtI-9V9EJZA","executionInfo":{"status":"ok","timestamp":1717571578632,"user_tz":-120,"elapsed":40374,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}},"outputId":"5e2a5431-efc0-45b4-b695-8840c7483402"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db4cdc6c752f472b873f40a96ffa8376"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}]},{"cell_type":"code","source":["messages = [\n","    {\"role\": \"user\", \"content\": \"Der Maschinenbau hat in Europa durch die Bildung der EU eine starke Erleichterung erhalten. Die einfachere Version des vorigen Satzes ist: Der Maschinenbau hat in Europa durch die Bildung der EU eine starke [MASK] erhalten. Provide 10 results:\"},\n","    {\"role\": \"assistant\", \"content\": '[\"Vereinfachung\", \"Entspannung\", \"Begünstigung\", \"Unterstützung\", \"Rückenwind\"]'},\n","]\n","\n","generation_args = {\n","    \"max_new_tokens\": 500,\n","    \"return_full_text\": False,\n","    \"temperature\": 0.0,\n","    \"do_sample\": False,\n","}\n","\n","llm_ls = LLMLexicalSimplifier(model, tokenizer, '{original_sentence} Die vereinfachte Version des vorigen Satzes ist: {sentence_with_complex_word_masked}', messages, '[MASK]', generation_args)\n","llm_output = llm_ls.generate_substitutions_for('heikel', 'Die politische Lage in Syrien ist [MASK].')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EUOiYbphOx9k","executionInfo":{"status":"ok","timestamp":1717572147671,"user_tz":-120,"elapsed":2663,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}},"outputId":"d8e60ca5-bf75-4ad6-91c4-9f9f6564fff4"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda for model inference.\n","Using mask token: \"[MASK]\".\n"]}]},{"cell_type":"code","source":["llm_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":55},"id":"P7jMPpDvPuLK","executionInfo":{"status":"ok","timestamp":1717572147674,"user_tz":-120,"elapsed":11,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}},"outputId":"af475cdf-c91e-4bc0-d40c-d00ab212dba3"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' [\"kompliziert\", \"bedrohlich\", \"instabil\", \"verschlechtert\", \"chaotisch\"]'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["!git status"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qFA54_voTr5Z","executionInfo":{"status":"ok","timestamp":1717572669378,"user_tz":-120,"elapsed":998,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}},"outputId":"2e819bc8-1f42-47d6-bdfa-d6f4eb2a0255"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch feature/benchmark-more-models\n","Your branch is up to date with 'origin/feature/benchmark-more-models'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   src/notebooks/evaluation_experimentation.ipynb\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}]},{"cell_type":"code","source":["!git stash list"],"metadata":{"id":"z5UHWT53TzH6","executionInfo":{"status":"ok","timestamp":1717572700752,"user_tz":-120,"elapsed":387,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["from benchmark_suite import BenchmarkSuite\n","from language import Language\n","suite = BenchmarkSuite(llm_ls, {Language.EN: '{original_sentence} The simpler version of the previous sentence is: {sentence_with_complex_word_masked}',\n","                                 Language.DE: '{original_sentence} Die vereinfachte Version des vorigen Satzes ist: {sentence_with_complex_word_masked}'})\n","suite.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":434},"id":"1KNZxflUTKi8","executionInfo":{"status":"error","timestamp":1717572548928,"user_tz":-120,"elapsed":7952,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}},"outputId":"02621bf7-b1e8-465c-a293-bfbf713b2cdb"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Benchmarking model on EN ...\n","Benchmarking model on BenchLSDataProvider...\n"]},{"output_type":"stream","name":"stderr","text":["\rBenchmarking:   0%|          | 0/929 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","Benchmarking:   0%|          | 3/929 [00:06<32:54,  2.13s/it]\n"]},{"output_type":"error","ename":"ZeroDivisionError","evalue":"division by zero","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-41-91470336f71c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m suite = BenchmarkSuite(llm_ls, {Language.EN: '{original_sentence} The simpler version of the previous sentence is: {sentence_with_complex_word_masked}',\n\u001b[1;32m      4\u001b[0m                                  Language.DE: '{original_sentence} Die vereinfachte Version des vorigen Satzes ist: {sentence_with_complex_word_masked}'})\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/drive/MyDrive/nlp_ss24/multilingual-lexical-simplification/src/benchmark_suite.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mbenchmark_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprovide_data_as_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf'{language.name}-{dataset.__class__.__name__}'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__benchmark_model_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchmark_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         results.to_csv('/content/drive/MyDrive/nlp_ss24/multilingual-lexical-simplification/data/'\n","\u001b[0;32m/content/drive/MyDrive/nlp_ss24/multilingual-lexical-simplification/src/benchmark_suite.py\u001b[0m in \u001b[0;36m__benchmark_model_on\u001b[0;34m(self, benchmark_data)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mpredicted_substitutions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestee_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_substitutions_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             sample_potential, sample_precision, sample_recall, sample_f1, sample_map_at_k, sample_potential_at_k, sample_accuracy_at_k_top_1 = Evaluator.evaluate(\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mground_truth_substitutions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_substitutions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             )\n","\u001b[0;32m/content/drive/MyDrive/nlp_ss24/multilingual-lexical-simplification/src/evaluator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(cls, ground_truth_substitutions, predicted_substitutions, k)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0msample_potential\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0msample_precision\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0msample_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_precision\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_substitutions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Calculate Recall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"]}]},{"cell_type":"markdown","source":["#BERT based"],"metadata":{"id":"dh_JhhgjEGLl"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2557,"status":"ok","timestamp":1717253600354,"user":{"displayName":"Luis Wiedmann","userId":"10160208935737893738"},"user_tz":-120},"id":"xuhHS5bjKWBo","outputId":"398234a4-caaf-4967-9db1-fb69061965d9"},"source":["from simple_bert_lexical_simplifier import SimpleBertLexicalSimplifier\n","bert_ls = SimpleBertLexicalSimplifier(model, tokenizer, bert_pattern, None)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda for model inference.\n","No exemplars provided, using zero-shot mode.\n"]}]},{"cell_type":"code","source":["from benchmark_suite import BenchmarkSuite\n","from language import Language\n","suite = BenchmarkSuite(bert_ls, {Language.EN: '{original_sentence} The simpler version of the previous sentence is: {sentence_with_complex_word_masked}',\n","                                 Language.DE: '{original_sentence} Die vereinfachte Version des vorigen Satzes ist: {sentence_with_complex_word_masked}'})\n","suite.run()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"em_Wxy2fGYOq","executionInfo":{"status":"error","timestamp":1717572509942,"user_tz":-120,"elapsed":3672,"user":{"displayName":"Tobias Lindenbauer","userId":"10361173233973415805"}},"outputId":"b9dfe236-6aae-4215-a76d-b55942f73fe6"},"execution_count":40,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-c810c9c2c7a8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbenchmark_suite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBenchmarkSuite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m suite = BenchmarkSuite(bert_ls, {Language.EN: '{original_sentence} The simpler version of the previous sentence is: {sentence_with_complex_word_masked}',\n\u001b[1;32m      4\u001b[0m                                  Language.DE: '{original_sentence} Die vereinfachte Version des vorigen Satzes ist: {sentence_with_complex_word_masked}'})\n\u001b[1;32m      5\u001b[0m \u001b[0msuite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/nlp_ss24/multilingual-lexical-simplification/src/benchmark_suite.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbench_ls_data_provider\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBenchLSDataProvider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_provider\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataProvider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgermaneval_data_provider\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGermanEvalDataProvider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlexmturk_data_provider\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLexMTurkDataProvider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnseval_data_provider\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNNSevalDataProvider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"db4cdc6c752f472b873f40a96ffa8376":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_89fd983aa28b49429ebbadbad49858e1","IPY_MODEL_15bd3262876244edb973433c02cb358a","IPY_MODEL_9fb6f554ee164820944359a6dafa44ae"],"layout":"IPY_MODEL_b217fcc4be144a67b32e7752ebae9e2c"}},"89fd983aa28b49429ebbadbad49858e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_375a8df081a84d85885146e76d80d467","placeholder":"​","style":"IPY_MODEL_129309f4318f4541a58d135f55810349","value":"Loading checkpoint shards: 100%"}},"15bd3262876244edb973433c02cb358a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3249b75df36474f81d75eb81b6753b9","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac6fb352152149409b9d9007d5419510","value":2}},"9fb6f554ee164820944359a6dafa44ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c406072ff5a64e919adcbf5e9ebf0258","placeholder":"​","style":"IPY_MODEL_12f748b910da4daa8800e3fa8b5bfbb7","value":" 2/2 [00:24&lt;00:00, 12.12s/it]"}},"b217fcc4be144a67b32e7752ebae9e2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"375a8df081a84d85885146e76d80d467":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"129309f4318f4541a58d135f55810349":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3249b75df36474f81d75eb81b6753b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac6fb352152149409b9d9007d5419510":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c406072ff5a64e919adcbf5e9ebf0258":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12f748b910da4daa8800e3fa8b5bfbb7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}