{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LexMTurkDataProvider():  \n",
    "    _filename = \"/home/luis/RCI/multilingual-lexical-simplification/data/lexmturk/lexmturk.txt\"  \n",
    "    def get_position(self, word, sentence):\n",
    "        \"\"\"\n",
    "        Returns the position of the word in the sentence. (Taken from germaneval_data_provider)\n",
    "        \"\"\"\n",
    "        words = sentence.split()\n",
    "        try:\n",
    "            index = words.index(word)\n",
    "        except:\n",
    "            index = next((i for i, s in enumerate(words) if word in s), None)\n",
    "        return index\n",
    "    \n",
    "    def process_line(self, line):\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        \n",
    "        sentence = parts[0]\n",
    "        complex_word = parts[1]\n",
    "        position = self.get_position(complex_word, sentence)\n",
    "\n",
    "        # Count the occurrences and group by counts simultaneously\n",
    "        count_dict = {}\n",
    "        for item in parts[2:]:\n",
    "            count_dict[item] = count_dict.get(item, 0) + 1\n",
    "\n",
    "        grouped_count_dict = {}\n",
    "        for item, count in count_dict.items():\n",
    "            if count in grouped_count_dict:\n",
    "                grouped_count_dict[count].append(item)\n",
    "            else:\n",
    "                grouped_count_dict[count] = [item]\n",
    "\n",
    "        # Sort by counts in descending order and create the final ranked dictionary\n",
    "        substitution_dict = {rank + 1: items for rank, (count, items) in enumerate(sorted(grouped_count_dict.items(), key=lambda x: x[0], reverse=True))}\n",
    "\n",
    "        return [sentence, complex_word, position, substitution_dict]\n",
    "    \n",
    "    def provide_data_as_numpy_array(self):\n",
    "        with open(self._filename, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        processed_lines = [self.process_line(line) for line in lines]\n",
    "        return np.array(processed_lines, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['\"In March 1992 , Linux version 0.95 was the first to be capable of running X. This large version number jump was due to a feeling that a version 1.0 with no major missing pieces was imminent .\"',\n",
       "        'pieces', 34,\n",
       "        {1: ['parts'], 2: ['bits'], 3: ['components'], 4: ['component', 'sections', 'elements', 'part', 'information', 'items']}],\n",
       "       ['Much of the water carried by these streams is diverted .',\n",
       "        'diverted', 9,\n",
       "        {1: ['redirected'], 2: ['rerouted'], 3: ['changed', 'moved'], 4: ['drawn away', 'turned', 'separated', 'switched', 'split', 'altered'], 5: ['led away', 'sent away', 'veered', 'channeled', 'deflected']}],\n",
       "       ['\"Harry also becomes the worthy possessor of the remaining Deathly Hallows : the Invisibility Cloak and the Resurrection Stone , hence becoming the true Master of Death .\"',\n",
       "        'possessor', 5,\n",
       "        {1: ['owner'], 2: ['holder'], 3: ['keeper'], 4: ['buyer', 'master', 'teacher']}],\n",
       "       ...,\n",
       "       ['\"Some features , however , are reminiscent of the Scout movement .\"',\n",
       "        'reminiscent', 6,\n",
       "        {1: ['similar'], 2: ['remindful'], 3: ['reminders'], 4: ['memorable'], 5: ['suggestive', 'familiar', 'similar to', 'like'], 6: ['similar ', 'comparitive', 'made', 'like features', 'piec', 'echoes', 'recolescent', 'telltale', 'replicas', 'nostalgic', 'similiar to', 'reminding of', 'similar to (remove of)', 'reminded of', 'evocative']}],\n",
       "       ['The Shinto pantheon alone consists of an uncountable number of kami .',\n",
       "        'consists', 4,\n",
       "        {1: ['is made'], 2: ['is made up'], 3: ['is'], 4: ['contains', 'is made '], 5: ['is made up of', 'is made up ', 'comprises'], 6: ['is composed', 'endures', 'made', 'composed', 'is made of', 'includes', 'has a lot', 'has', 'has most ', 'makes up', 'have', 'exists', 'is formed', 'belong', 'had']}],\n",
       "       ['This message earned Saddam a great deal of popularity in many sectors of the Arab world .',\n",
       "        'sectors', 11,\n",
       "        {1: ['parts'], 2: ['areas'], 3: ['sections', 'regions'], 4: ['places']}]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provider = LexMTurkDataProvider()\n",
    "provider.provide_data_as_numpy_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_filename = \"/home/luis/RCI/multilingual-lexical-simplification/data/lexmturk/lexmturk.txt\"\n",
    "with open(_filename, \"r\", encoding=\"utf-8\") as file:\n",
    "    lines = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position(word, sentence):\n",
    "    \"\"\"\n",
    "    Returns the position of the word in the sentence.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    try:\n",
    "        index = words.index(word)\n",
    "    except:\n",
    "        index = next((i for i, s in enumerate(words) if word in s), None)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Much of the water carried by these streams is diverted .\n",
      "diverted\n",
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: ['redirected'],\n",
       " 2: ['rerouted'],\n",
       " 3: ['changed', 'moved'],\n",
       " 4: ['drawn away', 'turned', 'separated', 'switched', 'split', 'altered'],\n",
       " 5: ['led away', 'sent away', 'veered', 'channeled', 'deflected']}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]\n",
    "parts = lines[1].strip().split(\"\\t\")\n",
    "sentence = parts[0]\n",
    "word = parts[1]\n",
    "position = get_position(word, sentence)\n",
    "\n",
    "print(sentence)\n",
    "print(word)\n",
    "print(position)\n",
    "\n",
    "# Count the occurrences and group by counts simultaneously\n",
    "count_dict = {}\n",
    "for item in parts[2:]:\n",
    "    count_dict[item] = count_dict.get(item, 0) + 1\n",
    "\n",
    "grouped_count_dict = {}\n",
    "for item, count in count_dict.items():\n",
    "    if count in grouped_count_dict:\n",
    "        grouped_count_dict[count].append(item)\n",
    "    else:\n",
    "        grouped_count_dict[count] = [item]\n",
    "\n",
    "# Sort by counts in descending order and create the final ranked dictionary\n",
    "ranked_dict = {rank + 1: items for rank, (count, items) in enumerate(sorted(grouped_count_dict.items(), key=lambda x: x[0], reverse=True))}\n",
    "\n",
    "ranked_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
